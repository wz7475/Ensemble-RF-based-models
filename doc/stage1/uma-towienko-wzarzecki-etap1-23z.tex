% !TeX spellcheck = pl_PL
\documentclass[10pt,a4paper]{article}

\usepackage{geometry}
\geometry{
	a4paper,
	total={170mm,257mm},
	left=20mm,
	top=20mm,
}

\usepackage{mathtools}
\usepackage{polski}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage[T1]{fontenc}
\usepackage{listings}
\usepackage{booktabs}
\usepackage{csvsimple}
\usepackage{geometry}
\usepackage{siunitx}
\usepackage{caption}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{float}
\usepackage{algpseudocode}
\usepackage{algorithm}


\captionsetup[table]{skip=10pt}

\sisetup{round-mode=places, round-precision=3, round-integer-to-decimal, scientific-notation = true, table-format = 1.3e2, table-number-alignment=center, group-separator={,}}


\title{Uczenie Maszynowe - projekt}
\author{Tomasz Owienko \and Wojciech Zarzecki}
\date{18.11.2023}


\begin{document}
	\maketitle
	
	\lstdefinestyle{code}{
		language=MATLAB,
		numbers=left,
		stepnumber=1,
		numbersep=10pt,
		tabsize=4,
		showspaces=false,
		showstringspaces=false
	}
	
	
	\lstset{basicstyle=\small,style=code}


\section{Cel projektu}\

Celem projektu jest implementacja zmodyfikowanej wersji algorytmu generowania lasu losowego, w której do generowania kolejnych drzew losowane są częściej elementy ze zbioru uczącego, na których dotychczasowy model się mylił.


Istotą metody lasu losowego w problemach regresji i klasyfikacji jest redukcja wariancji i nadmiernego dopasowania osiąganego przez pojedyncze drzewa decyzyjne. W klasycznych algorytmach generowania lasu losowego każde z $B$ drzew generowane jest na podstawie $\sqrt{B}$ przykładów ze zbioru trenującego wylosowanych ze zwracaniem, zazwyczaj ograniczonych (w problemach klasyfikacji) do $\sqrt{|D|}$ atrybutów wylosowanych bez zwracania, gdzie $D$ jest zbiorem atrybutów. Proces ten odbywa się w jednej iteracji - algorytm kończy pracę po wygenerowaniu $B$ drzew. 

Realizowany projekt zakłada wykorzystanie metod boostingowych do iteracyjnego poprawiania wyniku algorytmu generowania lasu losowego. Mechanizm losowania atrybutów do generowania kolejnych drzew zostanie zmodyfikowany przez wprowadzenie preferencji dla tych przykładów, na których dotychczasowy model się mylił.

// i co dalej?? Nowe drzewa dołączamy do lasu (de facto AdaBoost) czy tworzymy nowy las? (w drugim przypadku to nie jest boosting)

\section{Opis algorytmu}

TODO wzorki i coś o AdaBoost

\begin{algorithm}
	\caption{TrainRandomForest}\label{alg:caprf_train}
	\hspace*{\algorithmicindent} \textbf{Input}: $U\neq \emptyset$: zbiór przykładów trenujących, $C$: klasy przykładów, $D$: zbiór atrybutów wejściowych
\begin{algorithmic}[1]
	\State 	// przy założeniu, że każda iteracja to nowy las
	\State $F \gets \emptyset$
	\State $\hat C \gets \emptyset$
	\For{$i \gets 1$ to $N$} 
		\State $F_i \gets \emptyset$
		\For{$b \gets 1$ to $B$} 
			\State $U_b \gets $B elementów wylosowanych z $U$ z preferencją dla $\{u_j \in U: \hat C(u_j) \neq C(u_j)\}$
			\State $D_b \gets \sqrt{|D|} $ atrybutów wylosowanych z $D$ bez zwracania
			\State $f_b \gets $ drzewo decyzyjne wygenerowane na podstawie $C$, $U_b$ i $D_b$
			\State $F_i \gets F_i \cup \{f_b\}$
		\EndFor
		\State $F \gets $ las losowy $F$ ulepszony za pomocą $F_i$
		\For{$u \in U$}
			\State $\hat C \gets \hat C\ \cup$ PredictRandomForest($u$, $F$)
		\EndFor
		
	\EndFor
	\State \Return $F$
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{PredictRandomForest}\label{alg:rf_predict}
\hspace*{\algorithmicindent} \textbf{Input}: $x$: wektor wejściowy, $F$: nauczony las losowy
\begin{algorithmic}[1]
	\State $\hat C \gets \emptyset$
	\For{$f_b \in F$} 
		\State $\hat C \gets \hat C \cup {f_b(x)}$
	\EndFor
	\State \Return najczęstsza klasa z $\hat C$
\end{algorithmic}
\end{algorithm}

\section{Planowane eksperymenty}\

\section{Wykorzystywane zbiory danych}


\end{document}